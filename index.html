<!doctype html>
<html lang="en">

  <head>
    <meta charset="utf-8">

    <title>Conditional Dependency Networks</title>

    <meta name="description" content="spMI Exam, January 2016">
    <meta name="author" content="Martin Bjeldbak Madsen">

    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/theme/black.css" id="theme">

    <!-- Code syntax highlighting -->
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <!-- Printing and PDF exports -->
    <script>
var link = document.createElement( 'link' );
link.rel = 'stylesheet';
link.type = 'text/css';
link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>

    <!--[if lt IE 9]>
      <script src="lib/js/html5shiv.js"></script>
      <![endif]-->
  </head>

  <body>

    <div class="reveal">

      <!-- Any section element inside of this container is displayed as a slide -->
      <div class="slides">
        <section>
          <h3>Multi-Label Classification Using Conditional Dependency Networks (2011)</h3>
          <h4>Paper by Yuhong Guo and Suichen Gu</h4>
          <p>
          <small>Presentation by Martin Bjeldbak Madsen</small>
          <br />
          <small>January 15th, 2016</small>
          <br />
          <small>spMI exam, Aalborg University</small>
          </p>
        </section>

        <section data-markdown>
          <script type="text/template">
          ## Motivation
          How do we take advantage of dependencies between labels in multi-label classification techniques?

          Note:
          - Example: Movie may be comedy, action, probably not horror
          - So what is multi-label classification? Let's see...
          </script>
        </section>

        <section data-background="img/yosemite-full.jpg" data-markdown>
          <script type="text/template">
          ## Multi-label classification
          This image could belong to classes <br /> <em>{foliage, forest, mountain}</em>
          </script>
        </section>

        <section data-markdown>
          <script type="text/template">
          ## Multi-label classification
          <img data-src="img/multi-label.png" />
          - Superset of multi-class classification
          - Captures more than multi-class methods 
            - One vs. rest and one vs. one
          - Existing methods...
            - Have complicated learning and prediction phases
            - Require <em>many</em> classifiers (Bayesian), NP-hard (Bayesian, Markov random fields)
          - Is there a better way to model label independencies?

          Note:
          - What is multi-label classification? Compared to multi-class.
          - Example: Movie may be comedy, action, probably not horror
            - OAA, OAO will train competing classifiers
          - Existing classifiers: Multi-dim. Bayesian classifiers (de Waal) requires many classifiers, and it is NP-hard to identify optimal Bayesian network structure (Chickering). Conditional Markov random fields require inference for parameter learning, which is NP-hard (Heckerman)
          </script>
        </section>

        <section data-markdown>
          <script type="text/template">
          ## Conditional Dependency Networks

          Authors propose multi-label classification model, based on a cyclic directed graphical model known as dependency networks.


          Note:
          - Their approach consists of training such a conditional dep net, then using Gibbs sampling to infer labels
          - Only need to train $k$ classifiers, where $k$ is the number of labels
          - Requires some preliminaries...
          </script>
        </section>

        <section data-markdown>
          <script type="text/template">
            ## Overview
            - Preliminaries
            - Conditional Dependency Networks
            - Results
            - Conclusion & Thoughts
          </script>
        </section>

        <!-- PRELIMINARIES -->
        <section>
          <section data-markdown>
            <script type="text/template">
              ## Preliminaries
              1. Dependency networks
              2. Binary classification (training)
              3. Gibbs sampling (prediction)
            </script>
          </section>

        <section data-markdown>
          <script type="text/template">
          ### 1. Dependency Network
          <img data-src="img/bayesian-net.png" />
          <img data-src="img/dependency-net.png" height="155px" />
          - Cyclic, directed graphical model over random variables `$X = \left\{X_1, X_2, \dots, X_n \right\}$` and joint distribution `$\operatorname{p}\left(x\right)$`
          - Each node (random variable) conditioned on its parents
            - $p(\text{Age} \mid pa_{Age}) = p(\text{Age} \mid \text{Gender}, \text{Income})$
          - CPT for variables estimated independently
            - Using logistic regression, neural nets, etc...
          - Used for probabilistic inference using Gibbs sampling

          Note:
          - Not useful for modeling causation, unlike Bayesian
          - Computationally effecient to learn joint distribution of dep. net. 
          - Good for collaborative filtering (predicting preferences), as example shows. Also good for probabilistic queries.
          - In Microsoft paper, they estimate each variables distribution independently. Construct dependency network from (in)dependencies in these estimates.
          </script>
        </section>

        <section data-markdown>
          <script type="text/template">
          ### 2. Binary Classification
          <img data-src="img/sigmoid.png" height="300px" />
          <img data-src="img/svm2.png" height="300px" />
          - Logistic regression
          <br />
          - Support Vector Machines
          <br />
          <br/>
          <small>Image sources: [wikibooks.org](https://en.wikibooks.org/wiki/Artificial_Neural_Networks/Activation_Functions), [opencv.org](http://docs.opencv.org/2.4/doc/tutorials/ml/introduction_to_svm/introduction_to_svm.html)</small>

          Note:
          - Widely used models, both of which are implemented in this paper, as I show later
          - Logistic regression: I'll show how they implement this
          - These are two methods, there are others, such as neural nets, etc
          </script>
        </section>

        <section data-markdown>
          <script type="text/template">
          ### 3. Gibbs sampling
          - Iterative, random algorithm
          - Approximates joint distribution over time.
          - Used for approximate inference in Bayesian networks. Samples from joint distribution.

          Sampling procedure
          1. Initialize variables randomly. Suppose `$x = \langle x_1, x_2, \dots, x_K \rangle$`, with `$K$` variables and observed data `$\mathcal{D}$`
          2. Sample from conditional distribution of `$x_i$` given `$x_{\neg i}$`
            `$$
              x_i \sim \operatorname{p}\left(x_i \mid x_{\neg i}, \mathcal{D}\right) \text{ for } i = 1, \dots, k
              $$`
          3. Go back to step 2 until convergence.

          <!--
          As more samples are gathered, the joint distribution `$\operatorname{p}\left(x_1, x_2, \dots, x_K\right)$` is approached.
          -->

          Note:
          - Approximates by generating random samples from unknown joint distributions.
          - Works nicely, since dependency nets approximate joint distributions
          - Generates Markov chain, as only one variable is updated at a time
          </script>
        </section>

        </section>

        <!-- CONDITIONAL DEP NETWORKS -->
        <section>
          <section data-markdown>
            <script type="text/template">
              ## Conditional Dependency Networks
              1. Conditional Dependency Networks
              2. Training Conditional Dependency Networks
              3. Prediction with Conditional Dependency Networks
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
              ### 1. Conditional Dependency Networks
              <img data-src="img/conditional-dependency-network.png" width="70%">
              - Expanded version of dependency network
              - Fully connected between labels `$Y_1, Y_2, \dots, Y_K$`, conditioned on observed features `$X = \left\{X_1, X_2, \dots, X_n\right\}$`
              - `$Y_i$` binary variables, `$X_i$` continuous/discrete

              Note:
              - Label variables are interdependent on each other, conditioning on observation features X
              - Show example, four class variables
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
              ### 2. Training Conditional Dependency Networks
              - CPTs approximated using `$k$` probabilistic prediction models to learn `$\operatorname{p}\left(y_i = \pm 1 \mid x, y_{\neg i}, \theta_i\right)$`
                - SVM implementation possible
                - Logistic regression with `$L_2$` regularization
                `$$
              \max_{\theta_i} \sum_{l = 1}^t \log \operatorname{p}\left(y_i^l \mid x^l, y^l_{\neg i}, \theta_i\right) - \frac{\lambda}{2} \left(\theta_i^T \theta_i\right)
              $$` where the data set `$\mathcal{D}$` is defined by
              `$$
              \mathcal{D} = \left\{ x^l, y_1^l, \dots, y_k^l \right\}_{l = 1}^t
              $$`

              Note:
              - SAY FIRST: Better to approximate, due to a fully connected network CPTs having size 2^k (each variable has all other variables as its parents). This causes overfitting.
              - Thus better to approximate
              - $k$ is the number of labels (see prev slide)
              - `$\lambda$` trade-off parameter
              - Trained using convex optimization techniques
            </script>
          </section>

        <section data-markdown>
          <script type="text/template">
            ### 3. Prediction with Conditional Dependency Networks
            - Interested in finding most probably explanation (MPE)
            `$$
            y^* = \arg\max_y \operatorname{p}\left(y \mid x\right)
            $$`
            - Gibbs sampling used for approximate inference

            Note:
            - MPE shown to be NP-hard, even for acyclic dir. graph. (Bayesian networks)
          </script>
        </section>

        <section data-markdown>
          <script type="text/template">
            ### Gibbs Sampling Inference
            <img data-src="img/conditional-dependency-network.png" width="50%">
            1. Visit `$Y_1, Y_2, \dots, Y_k$` in random order
            2. Resample each variable until convergence (burn-in)
            3. Collect samples to recover approximate joint distribution `$\operatorname{p}\left(y_1, \dots, y_k \mid X \right)$` by finding samples with high values of
              `$$
                 \prod_i \operatorname{p}\left(y_i \mid x, y_{\neg i}, \theta_i \right)
               $$`

            Note:
            - As explained earlier...
            - Burn in usually 100 iterations
            - Sample collection 500 instances
          </script>
        </section>

        <section data-markdown>
          <script type="text/template">
          ### Example
          
          Note:
          - TODO: Example here if time
          </script>
        </section>

        </section>

        <!-- RESULTS -->
        <section>
          <section data-markdown>
            <script type="text/template">
              ## Results
              1. Measurement methods
              2. Competing models
              3. Results
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
            ### 1. Measurement methods
            1. Exact match Ratio
            2. Macro-F1
            3. Micro-F1

            Note:
            
            </script>
          </section>

          <section data-markdown style="text-align: left;">
            <script type="text/template">
            ### 1. Measurement methods: Exact Match

            Defined by
            `$$
            \operatorname{EX} = \frac{1}{n} \sum_{i = 1}^n \operatorname{I}\left[ L_i^{\text{pred}} = L_i^{\text{true}}\right],
            $$`
            where `$\operatorname{I}\left[S\right] = 1$` if statement `$S$` is true, `$0$` otherwise.
            
            Note:
            - Counts only exact matches between predicted label and true label
            
            </script>
          </section>

          <section data-markdown style="text-align: left;">
            <script type="text/template">
            ### 1. Measurement methods: Macro-F1
            - Harmonic mean of average recall and precision over multiple sets.

            - Computed by
              `$$
              \operatorname{Macro-avg}_\text{precision} = \frac
              {\sum_i^k \operatorname{Precision}_i}
              {k}
              $$`
              and
              `$$
              \operatorname{Macro-avg}_\text{recall} = \frac
              {\sum_i^k \operatorname{Recall}_i}
              {k}
              $$`
            - Then compute harmonic mean
            `$$
            \operatorname{Macro-F}_1 = \frac
            {\operatorname{Macro-avg}_\text{precision} \cdot \operatorname{Macro-avg}_\text{recall}}
            {\operatorname{Macro-avg}_\text{precision} + \operatorname{Macro-Avg}_\text{recall}}
            $$`

            Note:
            
            </script>
          </section>

          <section data-markdown style="text-align: left;">
            <script type="text/template">
            ### 1. Measurement methods: Micro-F1
            - Harmonic mean of the micro-average of precision and the micro-average of recall over multiple sets.

            - Computed by
              `$$
            \operatorname{Micro-avg}_\text{precision} = \frac
            {\sum \operatorname{TP}_i}
            {\sum \operatorname{TP}_i + \operatorname{FP}_i}
            $$`
            and
            `$$
            \operatorname{Micro-avg}_\text{recall} = \frac
            {\sum \operatorname{TP}_i}
            {\sum \operatorname{TP}_i + \operatorname{FN}_i}
            $$`
            - Then compute harmonic mean
            `$$
            \operatorname{Micro-F}_1 = \frac
            {\operatorname{Micro-avg}_\text{precision} \cdot \operatorname{Micro-avg}_\text{recall}}
            {\operatorname{Micro-avg}_\text{precision} + \operatorname{Micro-Avg}_\text{recall}}
            $$`

            Note:
            
            </script>
          </section>

          <section data-markdown style="text-align: left;">
            <script type="text/template">
            ### 2. Competing models
            - LR (baseline): One vs. rest classifier
            - MLKNN: modified `$k$`-nearest neighbor
            - CML: Collective multi-label classifcation (random fields)

            and
            - SVM (baseline)
            - SVM-HF: Heterogenerous feature kernels
            - RankSVM: Ranking based SVM
            
            Note:
            - One vs. rest: single classifier per class, with each only recognizing class and negative all others. No label dependencies
            - MLKNN: uses statistical information (MAP) from label sets of neighboring instances
            - CML: random fields
            - SVM-HF: Original SVM output scores added to documents in new columns (converted to binary)
            
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
            ### 3. Results (Exact Match)
            <img data-src="img/em-yeast.png" width="40%" />
            <img data-src="img/em-enron.png" width="40%" />

            <img data-src="img/em-emotion.png" width="40%" />
            <img data-src="img/em-medical.png" width="40%" />

            Note:
            - CDN-LR, CDN-SVM best on Yeast, Enron, Emotion, Medical
            - CDN-SVM tied on Genbase (not shown)

            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
            ### 3. Results (Macro-`$F_1$`)
            <img data-src="img/macro-yeast.png" width="40%" />
            <img data-src="img/macro-enron.png" width="40%" />

            <img data-src="img/macro-emotion.png" width="40%" />
            <img data-src="img/macro-medical.png" width="40%" />

            Note:
            - CDN-LR best on all datasets except enron
            - CDN-SVM best on all datasts except yeast, ties on genbase (not shown)

            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
            ### 3. Results (Micro-`$F_1$`)
            <img data-src="img/micro-yeast.png" width="40%" />
            <img data-src="img/micro-enron.png" width="40%" />

            <img data-src="img/micro-emotion.png" width="40%" />
            <img data-src="img/micro-medical.png" width="40%" />

            Note:
            - CDN-LR best on all datasets
            - CDN-SVM best on all datasets except genbase (not shown)

            </script>
          </section>

        </section>


        <!-- CONCLUSION -->
        <section>
          <section data-markdown>
            <script type="text/template">
            ## Conclusion
            - Extension to conditional dependency network allowing multi-label classification
            - Very simple to train w/ binary classifiers, straightforward to predict w/ Gibbs sampling
            - Amazing performance compared to well-document multi-label classification methods
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
            ## Constructive critisism
            - Algorithm was hard to read
            - Other binary classification models, or combination thereof?
            - Results with more samples
            - Usage for hierarchical classification techniques
            - Time to predict (convergence) not documented

            Note:
            - Hier class isn't multi-label, but has forced dependencies
              - Local approach: Start at classifier distinguishing between root levels. THen Classifier for every internal node. Datapoints classified by passing it down the classifiers, choosing the one with max probability
            </script>
          </section>
        </section>

        <!--
        <section data-markdown>
          <script type="text/template">
          </script>
        </section>
        -->

      </div>

    </div>

    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.js"></script>

    <script>

// Full list of configuration options available at:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({
  controls: true,
  progress: true,
  history: true,
  center: true,
  slideNumber: 'c/t',

  transition: 'slide', // none/fade/slide/convex/concave/zoom

    // Optional reveal.js plugins
  dependencies: [
  { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
  { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
  { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
  { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
  { src: 'plugin/zoom-js/zoom.js', async: true },
  { src: 'plugin/notes/notes.js', async: true },
  { src: 'plugin/math/math.js', async: true }
  ]
});

    </script>

  </body>
</html>
